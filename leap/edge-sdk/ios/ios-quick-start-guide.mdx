---
sidebar_position: 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Quick Start Guide

## Prerequisites

Make sure you have:

- Xcode 15.0 or later with Swift 5.9.
- An iOS project targeting **iOS 15.0+** (macOS 12.0+ or Mac Catalyst 15.0+ are also supported).
- A physical iPhone or iPad with at least 3 GB RAM for best performance. The simulator works for development but runs models much slower.

```text
iOS Deployment Target: 15.0
macOS Deployment Target: 12.0
```

:::warning
Always test on a real device before shipping. Simulator performance is not representative of
production behaviour.
:::

## Install the SDK

Choose your preferred installation method:

<Tabs>
<TabItem value="spm" label="Swift Package Manager (recommended)" default>

1. In Xcode choose **File -> Add Package Dependencies**.
2. Enter `https://github.com/Liquid4All/leap-ios.git`.
3. Select the `0.7.0` release (or newer).
4. Add the **`LeapSDK`** product to your app target.
5. (Optional) Add **`LeapModelDownloader`** if you plan to download model bundles at runtime.

:::info
The constrained-generation macros (`@Generatable`, `@Guide`) ship inside the `LeapSDK` product. No additional package is required.
:::

</TabItem>
<TabItem value="cocoapods" label="CocoaPods">

1. Add the pod to your `Podfile`:
  ```ruby
  pod 'Leap-SDK', '~> 0.7.0'
  # Optional: pod 'Leap-Model-Downloader', '~> 0.7.0'
  ```

2. Run `pod install`
3. Reopen the `.xcworkspace`.

</TabItem>
<TabItem value="manual" label="Manual installation">

1. Download `LeapSDK.xcframework.zip` (and optionally `LeapModelDownloader.xcframework.zip`) from the [GitHub releases](https://github.com/Liquid4All/leap-ios/releases).
2. Unzip and drag the XCFramework(s) into Xcode.
3. Set the Embed setting to **Embed & Sign** for each framework.

</TabItem>
</Tabs>

## Getting and Loading Models

The SDK supports two methods for loading models.
- GGUF manifests. This is the recommended for new projects because of superior inference performance and good default generation parameters.
- Executorch bundles (legacy)

<Tabs>
<TabItem value="automatic" label="GGUF manifests (recommended)" default>

The LeapSDK supports direct downloads of GGUF files from the [LeapBundles repository](https://huggingface.co/LiquidAI/LeapBundles/tree/main). You can point directly to a remote JSON manifest, and the SDK will automatically download the necessary GGUF files along with generation parameters for optimal performance.

```swift
import LeapSDK
import LeapModelDownloader

// Download model from manifest
let manifestURL: URL = URL(string: "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/leap/Q5_K_M.json")!

// Initialize downloader
let downloader = ModelDownloader()

// Call `downloadModelFromManifest` to download & cache a model and related files from given manifest URL
let _ = try await downloader.downloadModelFromManifest(manifestURL) { progress, speed in
    // progress: Double (0...1)
    // speed: bytes per second
}

```

Use `Leap.load(manifestURL)` inside an async context, to load the model runner from the GGUF manifest.

### Example with GGUF manifests (recommended)
```swift
import LeapSDK
import LeapModelDownloader
import Combine

@MainActor
final class ChatViewModel: ObservableObject {
    @Published var isLoading = false
    @Published var conversation: Conversation?

    private var modelRunner: ModelRunner?
    private var generationTask: Task<Void, Never>?

    func loadModel() async {

        isLoading = true
        defer { isLoading = false }

        do {
            // Download model from manifest
            let manifestURL: URL = URL(string: "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/leap/Q5_K_M.json")!

            // Initialize downloader
            let downloader = ModelDownloader()

            // Call `downloadModelFromManifest` to download & cache a model and related files from given manifest URL
            let _ = try await downloader.downloadModelFromManifest(manifestURL) { progress, speed in
                // progress: Double (0...1)
                // speed: bytes per second
            }

            // ModelRunner is initialized with manifestURL.
            // Even if you skipped direct downloading with ModelDownloader,
            // ModelRunner is able to handle download itself.
            let modelRunner = try await Leap.load(manifestURL: manifestURL) { _, _ in }

            conversation = modelRunner.createConversation(systemPrompt: "You are a helpful travel assistant.")
            self.modelRunner = modelRunner

        } catch {
            print("Failed to load model: \(error)")
        }
    }

    func send(_ text: String) {
        guard let conversation else { return }

        generationTask?.cancel()

        let userMessage = ChatMessage(role: .user, content: [.text(text)])

        generationTask = Task { [weak self] in
            do {
                for try await response in conversation.generateResponse(
                    message: userMessage,
                    generationOptions: GenerationOptions(temperature: 0.7)
                ) {
                    self?.handle(response)
                }
            } catch {
                print("Generation failed: \(error)")
            }
        }
    }

    func stopGeneration() {
        generationTask?.cancel()
    }

    @MainActor
    private func handle(_ response: MessageResponse) {
        switch response {
        case .chunk(let delta):
            print(delta, terminator: "") // Update UI binding here
        case .reasoningChunk(let thought):
            print("Reasoning:", thought)
        case .audioSample(let samples, let sr):
            print("Received audio samples \(samples.count) at sample rate \(sr)")
        case .functionCall(let calls):
            print("Requested calls: \(calls)")
        case .complete(let completion):
            if let stats = completion.stats {
                print("Finished with \(stats.totalTokens) tokens")
            }
            let text = completion.message.content.compactMap { part -> String? in
                if case .text(let value) = part { return value }
                return nil
            }.joined()
            print("Final response:", text)
            // completion.message.content may also include `.audio` entries you can persist or replay
        }
    }
}
```

</TabItem>
<TabItem value="manual" label="Executorch bundles (legacy)">

Browse the [Leap Model Library](https://leap.liquid.ai/models) and download a `.bundle` file for the model/quantization you want. `.bundle` packages contain metadata plus assets for the ExecuTorch backend.

You can either:

- **Ship it with the app** - drag the bundle into your Xcode project and ensure it is added to the main target.
- **Download at runtime** - use `LeapModelDownloader` to fetch bundles on demand.

<details>
<summary>Alternative: Download at runtime</summary>

```swift
import LeapModelDownloader

let model = await LeapDownloadableModel.resolve(
  modelSlug: "lfm2-350m-enjp-mt",
  quantizationSlug: "lfm2-350m-enjp-mt-20250904-8da4w"
)

if let model {
  let downloader = ModelDownloader()
  downloader.requestDownloadModel(model)

  let status = await downloader.queryStatus(model)
  switch status {
  case .downloaded:
    let bundleURL = downloader.getModelFile(model)
    try await runModel(at: bundleURL)
  case .downloadInProgress(let progress):
    print("Progress: \(Int(progress * 100))%")
  case .notOnLocal:
    print("Waiting for download...")
  }
}
```

</details>

Use `Leap.load(url:options:)` inside an async context. Passing a `.bundle` loads the model through the ExecuTorch backend.

### Example with Executorch bundles (legacy)
```swift
import LeapSDK

@MainActor
final class ChatViewModel: ObservableObject {
  @Published var isLoading = false
  @Published var conversation: Conversation?

  private var modelRunner: ModelRunner?
  private var generationTask: Task<Void, Never>?

  func loadModel() async {
    guard let bundleURL = Bundle.main.url(forResource: "LFM2-350-ENJP-MT", withExtension: "bundle") else {
      assertionFailure("Model bundle missing")
      return
    }

    isLoading = true
    defer { isLoading = false }

    do {
      modelRunner = try await Leap.load(url: bundleURL)
      conversation = modelRunner?.createConversation(systemPrompt: "You are a helpful travel assistant.")
    } catch {
      print("Failed to load model: \(error)")
    }
  }

  func send(_ text: String) {
    guard let conversation else { return }

    generationTask?.cancel()

    let userMessage = ChatMessage(role: .user, content: [.text(text)])

    generationTask = Task { [weak self] in
      do {
        for try await response in conversation.generateResponse(
          message: userMessage,
          generationOptions: GenerationOptions(temperature: 0.7)
        ) {
          await self?.handle(response)
        }
      } catch {
        print("Generation failed: \(error)")
      }
    }
  }

  func stopGeneration() {
    generationTask?.cancel()
  }

  @MainActor
  private func handle(_ response: MessageResponse) {
    switch response {
    case .chunk(let delta):
      print(delta, terminator: "") // Update UI binding here
    case .reasoningChunk(let thought):
      print("Reasoning:", thought)
    case .audioSample(let samples, let sr):
      audioRenderer.enqueue(samples, sampleRate: sr)
    case .functionCall(let calls):
      print("Requested calls: \(calls)")
    case .complete(let completion):
      if let stats = completion.stats {
        print("Finished with \(stats.totalTokens) tokens")
      }
      let text = completion.message.content.compactMap { part -> String? in
        if case .text(let value) = part { return value }
        return nil
      }.joined()
      print("Final response:", text)
      // completion.message.content may also include `.audio` entries you can persist or replay
    }
  }
}
```

Need custom runtime settings (threads, context size, GPU layers)? Pass a `LiquidInferenceEngineOptions` value:

```swift
let options = LiquidInferenceEngineOptions(
  bundlePath: bundleURL.path,
  cpuThreads: 6,
  contextSize: 8192,
  nGpuLayers: 8
)
let runner = try await Leap.load(url: bundleURL, options: options)
```

</TabItem>
</Tabs>

## Stream responses

`send(_:)` (shown above) launches a `Task` that consumes the `AsyncThrowingStream` returned by `Conversation.generateResponse`. Each `MessageResponse` case maps to UI updates, tool execution, or completion metadata. Cancel the task manually (for example via `stopGeneration()`) to interrupt generation early. You can also observe `conversation.isGenerating` to disable UI controls while a request is in flight.

### Send images and audio (optional)

When the loaded model ships with multimodal weights (and companion files were detected), you can mix text, image, and audio content in the same message:

```swift
let message = ChatMessage(
  role: .user,
  content: [
    .text("Describe what you see."),
    .image(jpegData)  // Data containing JPEG bytes
  ]
)

let audioMessage = ChatMessage(
  role: .user,
  content: [
    .text("Transcribe and summarize this clip."),
    .audio(wavData)  // Data containing WAV bytes
  ]
)

let pcmMessage = ChatMessage(
  role: .user,
  content: [
    .text("Give feedback on my pronunciation."),
    ChatMessageContent.fromFloatSamples(samples, sampleRate: 16000)
  ]
)
```

## Add tool results back to the history

```swift
let toolMessage = ChatMessage(
  role: .tool,
  content: [
    .text("{\"temperature\":72,\"conditions\":\"sunny\"}"),
    .audio(toolAudioData) // Optional: return audio bytes from your tool
  ]
)

guard let current = conversation else { return }
let updatedHistory = current.history + [toolMessage]
conversation = current.modelRunner.createConversationFromHistory(
  history: updatedHistory
)
```

## Next steps

- Learn how to expose structured JSON outputs with the [`@Generatable` macros](./constrained-generation).
- Wire up tools and external APIs with [Function Calling](./function-calling).
- Compare on-device and cloud behaviour in [Cloud AI Comparison](./cloud-ai-comparison).

You now have a project that loads an on-device model, streams responses, and is ready for advanced features like structured output and tool use.
